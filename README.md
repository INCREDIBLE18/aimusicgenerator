# ğŸ¶ AI Music Composer (Advanced ML) â€“ RNN + Transformer + GAN (+RL)

This project implements the full pipeline you requested:

```
Dataset (MIDI files)
       â¬‡
Preprocessing (Pitch, Step, Duration â†’ Tokens)
       â¬‡
Baseline Model (RNN/LSTM) â†’ Generates simple melody
       â¬‡
Transformer Model â†’ Adds structure & long-term patterns
       â¬‡
GAN (Generator + Discriminator) â†’ Refines for realism
       â¬‡
(Optional RL Fine-tuning) â†’ Improves harmony/quality
       â¬‡
Generated Music (MIDI)
       â¬‡
Visualization + Streamlit UI (Play, Download)
```

> Note: The GAN and RL components are provided as **working skeletons** to keep the repo concise. You can train the RNN/Transformer end-to-end out of the box, then optionally explore adversarial refinement and policy-gradients.

## ğŸ“¦ Quickstart

1) **Install deps** (Python 3.10+ recommended):
```bash
pip install -r requirements.txt
```

2) **Add dataset**: Place `.mid` files into `midi_songs/`.

3) **Preprocess**:
```bash
python -m src.data.preprocess --midi_dir midi_songs --out_dir outputs --min_notes 200
```

4) **Train RNN baseline**:
```bash
python -m src.train_rnn --config config.yaml
```

5) **Train Transformer**:
```bash
python -m src.train_transformer --config config.yaml
```

6) **(Optional) Train GAN**:
```bash
python -m src.train_gan --config config.yaml
```

7) **(Optional) RL fine-tune**:
```bash
python -m src.fine_tune_rl --config config.yaml
```

8) **Generate MIDI** (choose model type: `rnn` or `transformer`):>
```bash
python -m src.generate --model_type rnn --checkpoint outputs/rnn/best.keras --out outputs/generated_rnn.mid
python -m src.generate --model_type transformer --checkpoint outputs/transformer/best.keras --out outputs/generated_transformer.mid
```

9) **Run UI** (Streamlit):
```bash
streamlit run src/ui/app.py
```

### ğŸ“ Repo Layout
```
ai-music-aml/
â”œâ”€ midi_songs/                 # put your .mid files here
â”œâ”€ outputs/                    # checkpoints + processed data + generated MIDI
â”œâ”€ src/
â”‚  â”œâ”€ data/
â”‚  â”‚  â””â”€ preprocess.py         # MIDI â†’ events â†’ token sequences
â”‚  â”œâ”€ models/
â”‚  â”‚  â”œâ”€ rnn.py                # LSTM baseline
â”‚  â”‚  â”œâ”€ transformer.py        # Transformer model
â”‚  â”‚  â””â”€ gan.py                # Simple sequence GAN (skeleton)
â”‚  â”œâ”€ rl/
â”‚  â”‚  â””â”€ reward.py             # simple music rewards + RL fine-tune utils
â”‚  â”œâ”€ utils/
â”‚  â”‚  â”œâ”€ midi.py               # MIDI <-> events/tokens utilities
â”‚  â”‚  â””â”€ dataio.py             # dataset loading helpers
â”‚  â”œâ”€ ui/
â”‚  â”‚  â””â”€ app.py                # Streamlit UI
â”‚  â”œâ”€ train_rnn.py
â”‚  â”œâ”€ train_transformer.py
â”‚  â”œâ”€ train_gan.py
â”‚  â”œâ”€ fine_tune_rl.py
â”‚  â””â”€ generate.py
â”œâ”€ config.yaml
â”œâ”€ requirements.txt
â””â”€ README.md
```

## ğŸ”¬ Research Inspiration
- Eck & Schmidhuber (2002): LSTM for music improv.
- Boulanger-Lewandowski (2012): RNN-RBM for polyphonic music.
- Mogren (2016): C-RNN-GAN (GAN for music).
- Huang et al. (2018): Music Transformer.
- Hadjeres et al. (2017): DeepBach.

---

**Tip:** Start with `train_rnn.py` (fastest), verify generation, then move to `train_transformer.py`. After that, explore `train_gan.py` and `fine_tune_rl.py` to turn this into a full **Advanced ML** showcase.
